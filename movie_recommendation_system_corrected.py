# -*- coding: utf-8 -*-
"""movie_recommendation_system_corrected.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xdOr6B2FtnFJ8vmMR-KAP4jtNBE0Epp6

# Netflix Movie Search API - Dataset Analysis & Recommendation System

This notebook analyzes the movies dataset, tests the existing search implementation, and builds a recommendation system using content-based filtering and matrix factorization.
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("rounakbanik/the-movies-dataset")

print("Path to dataset files:", path)

"""## 1. Setup and Data Loading"""

# Standard imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ML imports for recommendation system
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score, roc_curve
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD, NMF
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from scipy.sparse import csr_matrix
import random
from datetime import datetime

# Set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

# Set plotting style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

print("All libraries imported successfully!")

# Load the movie data
print("Loading movies dataset...")
movies_df = pd.read_csv('movie_data/movies_metadata.csv', low_memory=False)

print(f"Dataset shape: {movies_df.shape}")
print(f"Columns: {movies_df.columns.tolist()}")
movies_df.head()

"""## 2. Data Cleaning and Preprocessing"""

# Clean and process the data
print("Cleaning data...")

# Convert release_date to datetime
movies_df['release_date'] = pd.to_datetime(movies_df['release_date'], errors='coerce')
movies_df['release_year'] = movies_df['release_date'].dt.year

# Convert numeric columns
movies_df['vote_average'] = pd.to_numeric(movies_df['vote_average'], errors='coerce')
movies_df['vote_count'] = pd.to_numeric(movies_df['vote_count'], errors='coerce')
movies_df['budget'] = pd.to_numeric(movies_df['budget'], errors='coerce')
movies_df['revenue'] = pd.to_numeric(movies_df['revenue'], errors='coerce')
movies_df['runtime'] = pd.to_numeric(movies_df['runtime'], errors='coerce')

# Process genres
def parse_genres(genres_str):
    try:
        if pd.isna(genres_str):
            return []
        genres_list = eval(genres_str)
        return [genre['name'] for genre in genres_list]
    except:
        return []

movies_df['genres_list'] = movies_df['genres'].apply(parse_genres)

# Filter out movies with missing essential data
movies_clean = movies_df.dropna(subset=['title', 'release_date', 'vote_average'])
movies_clean = movies_clean[movies_clean['vote_count'] >= 10]  # At least 10 votes

print(f"After cleaning: {movies_clean.shape[0]} movies")
print(f"Date range: {movies_clean['release_year'].min()} - {movies_clean['release_year'].max()}")
print(f"Average rating: {movies_clean['vote_average'].mean():.2f}")
print(f"Average runtime: {movies_clean['runtime'].mean():.0f} minutes")

"""## 3. Testing Existing Search Implementation"""

# Function to extract genre from query text (from original app.py)
def extract_genres(query):
    genre_map = {
        'horror': ['horror'],
        'action': ['action'],
        'comedy': ['comedy', 'romance'],
        'drama': ['drama']
    }

    words = query.lower().split()
    possible_genres = []
    for word in words:
        if word in genre_map:
            possible_genres.extend(genre_map[word])
    return list(set(possible_genres))

# Search function that mimics the Flask API endpoint
def search_movies(query, movies_df):
    if not query:
        return {'error': 'Query is required'}

    target_genres = extract_genres(query)

    # Filter movies based on genres
    results = []
    for index, movie in movies_df.iterrows():
        if movie['genres_list']:  # Use the parsed genres list
            if any(genre.lower() in [tg.lower() for tg in target_genres] for genre in movie['genres_list']):
                results.append({
                    'title': movie['title'],
                    'release_date': str(movie['release_date']),
                    'rating': movie['vote_average']
                })

    return {'results': results}

# Test queries from the readme
test_queries = [
    "A horror movie suitable for teenagers in the 12-15 age range",
    "A spooky movie suitable for teenagers in the 12-15 age range",
    "War commandos",
    "action movies",
    "comedy films",
    "drama movies"
]

print("="*60)
print("TESTING EXISTING MOVIE SEARCH IMPLEMENTATION")
print("="*60)

for query in test_queries:
    print(f"\nQuery: '{query}'")
    print("-" * 40)

    results = search_movies(query, movies_clean)

    if 'error' in results:
        print(f"Error: {results['error']}")
    else:
        num_results = len(results['results'])
        print(f"Found {num_results} results")

        # Show first 3 results
        for i, movie in enumerate(results['results'][:3]):
            release_date = movie['release_date'][:10] if movie['release_date'] != 'NaT' else 'Unknown'
            print(f"  {i+1}. {movie['title']} ({release_date}) - Rating: {movie['rating']}")

        if num_results > 3:
            print(f"  ... and {num_results - 3} more results")

"""## 4. Load Additional Data for Recommendations"""

# Load additional datasets for recommendation system
print("Loading additional datasets...")

# Load credits data (cast and crew information)
credits_df = pd.read_csv('movie_data/credits.csv')
print(f"Credits dataset shape: {credits_df.shape}")

# Load keywords data
keywords_df = pd.read_csv('movie_data/keywords.csv')
print(f"Keywords dataset shape: {keywords_df.shape}")

# Load ratings data
ratings_df = pd.read_csv('movie_data/ratings_small.csv')  # Using small version for performance
print(f"Ratings dataset shape: {ratings_df.shape}")

print("\nDataset previews:")
print("\nCredits columns:", credits_df.columns.tolist())
print("Keywords columns:", keywords_df.columns.tolist())
print("Ratings columns:", ratings_df.columns.tolist())

# Process and merge the datasets
import json

def parse_json_field(x):
    """Parse JSON-like fields safely"""
    try:
        if pd.isna(x):
            return []
        # Replace single quotes with double quotes for valid JSON
        json_str = str(x).replace("'", '"')
        data = json.loads(json_str)
        if isinstance(data, list):
            return [item.get('name', '') for item in data if isinstance(item, dict)]
        return []
    except:
        return []

def get_top_cast(cast_str, n=5):
    """Extract top N cast members"""
    try:
        if pd.isna(cast_str):
            return []
        # Replace single quotes with double quotes for valid JSON
        json_str = str(cast_str).replace("'", '"')
        cast_list = json.loads(json_str)
        return [actor['name'] for actor in cast_list[:n]]
    except:
        return []

def get_director(crew_str):
    """Extract director from crew data"""
    try:
        if pd.isna(crew_str):
            return ''
        # Replace single quotes with double quotes for valid JSON
        json_str = str(crew_str).replace("'", '"')
        crew_list = json.loads(json_str)
        for person in crew_list:
            if person.get('job') == 'Director':
                return person.get('name', '')
        return ''
    except:
        return ''

print("Processing additional data...")

# Process credits data
credits_df['cast_list'] = credits_df['cast'].apply(lambda x: get_top_cast(x, 5))
credits_df['director'] = credits_df['crew'].apply(get_director)

# Process keywords data
keywords_df['keywords_list'] = keywords_df['keywords'].apply(parse_json_field)

# Fix data type mismatches before merging
print("Fixing data types for merging...")

# Convert id columns to consistent integer type
movies_clean = movies_clean.copy()
movies_clean['id'] = pd.to_numeric(movies_clean['id'], errors='coerce')
credits_df['id'] = pd.to_numeric(credits_df['id'], errors='coerce')
keywords_df['id'] = pd.to_numeric(keywords_df['id'], errors='coerce')

# Remove rows with invalid IDs
movies_clean = movies_clean.dropna(subset=['id'])
credits_df = credits_df.dropna(subset=['id'])
keywords_df = keywords_df.dropna(subset=['id'])

# Convert to int
movies_clean['id'] = movies_clean['id'].astype(int)
credits_df['id'] = credits_df['id'].astype(int)
keywords_df['id'] = keywords_df['id'].astype(int)

print(f"After type conversion:")
print(f"Movies ID type: {movies_clean['id'].dtype}")
print(f"Credits ID type: {credits_df['id'].dtype}")
print(f"Keywords ID type: {keywords_df['id'].dtype}")

# Merge all datasets
print("Merging datasets...")
movies_enhanced = movies_clean.merge(credits_df[['id', 'cast_list', 'director']], on='id', how='left')
movies_enhanced = movies_enhanced.merge(keywords_df[['id', 'keywords_list']], on='id', how='left')

# Fill NaN values
movies_enhanced['cast_list'] = movies_enhanced['cast_list'].fillna('').apply(lambda x: x if isinstance(x, list) else [])
movies_enhanced['keywords_list'] = movies_enhanced['keywords_list'].fillna('').apply(lambda x: x if isinstance(x, list) else [])
movies_enhanced['director'] = movies_enhanced['director'].fillna('')
movies_enhanced['overview'] = movies_enhanced['overview'].fillna('')

print(f"Enhanced movies dataset shape: {movies_enhanced.shape}")
print(f"Sample enhanced movie data:")
sample_movie = movies_enhanced.iloc[0]
print(f"Title: {sample_movie['title']}")
print(f"Genres: {sample_movie['genres_list']}")
print(f"Cast: {sample_movie['cast_list']}")
print(f"Director: {sample_movie['director']}")
print(f"Keywords: {sample_movie['keywords_list'][:5]}")  # Show first 5 keywords

"""## 5. Content-Based Recommendation System"""

class ContentBasedRecommender:
    def __init__(self, movies_df):
        self.movies_df = movies_df.copy()
        self.tfidf_vectorizer = None
        self.content_matrix = None
        self.movie_indices = None
        self._prepare_content_features()
        self._build_content_matrix()

    def _prepare_content_features(self):
        """Combine all content features into a single text representation"""
        print("Preparing content features...")

        def clean_text(text):
            if isinstance(text, list):
                return ' '.join([str(item).lower().replace(' ', '') for item in text])
            return str(text).lower().replace(' ', '') if text else ''

        # Prepare content features
        self.movies_df['content_features'] = (
            self.movies_df['genres_list'].apply(clean_text) + ' ' +
            self.movies_df['keywords_list'].apply(clean_text) + ' ' +
            self.movies_df['cast_list'].apply(clean_text) + ' ' +
            self.movies_df['director'].apply(clean_text) + ' ' +
            self.movies_df['overview'].fillna('').str.lower()
        )

        # Create mapping from movie title to index
        self.movie_indices = pd.Series(self.movies_df.index, index=self.movies_df['title']).to_dict()

        print(f"Prepared content features for {len(self.movies_df)} movies")

    def _build_content_matrix(self):
        """Build TF-IDF matrix from content features"""
        print("Building TF-IDF content matrix...")

        # Use TF-IDF to vectorize content features
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

        self.content_matrix = self.tfidf_vectorizer.fit_transform(self.movies_df['content_features'])
        print(f"Content matrix shape: {self.content_matrix.shape}")

    def get_recommendations(self, movie_title, n_recommendations=10):
        """Get content-based recommendations for a movie"""
        if movie_title not in self.movie_indices:
            return f"Movie '{movie_title}' not found in database"

        # Get movie index
        movie_idx = self.movie_indices[movie_title]

        # Calculate cosine similarity
        movie_vector = self.content_matrix[movie_idx]
        similarities = cosine_similarity(movie_vector, self.content_matrix).flatten()

        # Get top similar movies (excluding the movie itself)
        similar_indices = similarities.argsort()[::-1][1:n_recommendations+1]

        # Prepare recommendations
        recommendations = []
        for idx in similar_indices:
            movie = self.movies_df.iloc[idx]
            recommendations.append({
                'title': movie['title'],
                'genres': movie['genres_list'],
                'rating': movie['vote_average'],
                'year': int(movie['release_year']) if pd.notna(movie['release_year']) else 'Unknown',
                'similarity_score': similarities[idx],
                'director': movie['director'],
                'cast': movie['cast_list'][:3]  # Top 3 cast members
            })

        return recommendations

    def get_recommendations_by_query(self, query, n_recommendations=10):
        """Get recommendations based on a text query"""
        print(f"Getting content-based recommendations for query: '{query}'")

        # Transform query using the same TF-IDF vectorizer
        query_vector = self.tfidf_vectorizer.transform([query.lower()])

        # Calculate similarities
        similarities = cosine_similarity(query_vector, self.content_matrix).flatten()

        # Get top similar movies
        similar_indices = similarities.argsort()[::-1][:n_recommendations]

        # Prepare recommendations
        recommendations = []
        for idx in similar_indices:
            if similarities[idx] > 0:  # Only include movies with positive similarity
                movie = self.movies_df.iloc[idx]
                recommendations.append({
                    'title': movie['title'],
                    'genres': movie['genres_list'],
                    'rating': movie['vote_average'],
                    'year': int(movie['release_year']) if pd.notna(movie['release_year']) else 'Unknown',
                    'similarity_score': similarities[idx],
                    'director': movie['director'],
                    'overview': movie['overview'][:100] + '...' if len(movie['overview']) > 100 else movie['overview']
                })

        return recommendations

# Initialize content-based recommender
print("="*60)
print("BUILDING CONTENT-BASED RECOMMENDATION SYSTEM")
print("="*60)

content_recommender = ContentBasedRecommender(movies_enhanced)

# Test content-based recommendations
print("Testing Content-Based Recommendations")
print("="*50)

# Test with specific movie
test_movie = "Toy Story"
print(f"\nRecommendations similar to '{test_movie}':")
print("-" * 40)
recs = content_recommender.get_recommendations(test_movie, 5)
for i, rec in enumerate(recs, 1):
    print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
    print(f"   Genres: {', '.join(rec['genres'][:3])}")
    print(f"   Similarity: {rec['similarity_score']:.3f}")
    print()

# Test with text queries from the original search
test_queries = [
    "horror movie suitable for teenagers",
    "spooky scary thriller",
    "action adventure",
    "romantic comedy",
    "war military"
]

for query in test_queries:
    print(f"\nContent-based results for: '{query}'")
    print("-" * 40)
    recs = content_recommender.get_recommendations_by_query(query, 3)
    if recs:
        for i, rec in enumerate(recs, 1):
            print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
            print(f"   Similarity: {rec['similarity_score']:.3f}")
    else:
        print("No recommendations found")
    print()

"""## 6. Collaborative Filtering Recommendation System"""

# Prepare ratings data for collaborative filtering
print("Preparing ratings data for collaborative filtering...")
print(f"Ratings dataset shape: {ratings_df.shape}")
print(f"Unique users: {ratings_df['userId'].nunique()}")
print(f"Unique movies: {ratings_df['movieId'].nunique()}")
print(f"Rating range: {ratings_df['rating'].min()} - {ratings_df['rating'].max()}")

# Check overlap between ratings and movies datasets
ratings_movie_ids = set(ratings_df['movieId'].unique())
movies_movie_ids = set(movies_enhanced['id'].astype(str).astype(int).unique())
common_movies = ratings_movie_ids.intersection(movies_movie_ids)

print(f"\nDataset overlap:")
print(f"Movies in ratings: {len(ratings_movie_ids)}")
print(f"Movies in metadata: {len(movies_movie_ids)}")
print(f"Common movies: {len(common_movies)}")

# Filter ratings to only include movies we have metadata for
ratings_filtered = ratings_df[ratings_df['movieId'].isin(common_movies)].copy()
print(f"Filtered ratings shape: {ratings_filtered.shape}")

# Create user-item matrix
print("Creating user-item rating matrix...")
user_item_matrix = ratings_filtered.pivot_table(
    index='userId',
    columns='movieId',
    values='rating'
).fillna(0)

print(f"User-item matrix shape: {user_item_matrix.shape}")
print(f"Sparsity: {(user_item_matrix == 0).sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.3f}")

class CollaborativeFilteringRecommender:
    def __init__(self, user_item_matrix, movies_df, ratings_df):
        self.user_item_matrix = user_item_matrix
        self.movies_df = movies_df
        self.ratings_df = ratings_df
        self.svd_model = None
        self.movie_id_to_title = {}
        self.title_to_movie_id = {}
        self._prepare_mappings()
        self._build_model()

    def _prepare_mappings(self):
        """Create mappings between movie IDs and titles"""
        # Create movie ID to title mapping
        for _, movie in self.movies_df.iterrows():
            movie_id = int(movie['id'])
            if movie_id in self.user_item_matrix.columns:
                self.movie_id_to_title[movie_id] = movie['title']
                self.title_to_movie_id[movie['title']] = movie_id

    def _build_model(self):
        """Build SVD model for collaborative filtering"""
        print("Building SVD model for collaborative filtering...")

        # Convert to sparse matrix for efficiency
        sparse_matrix = csr_matrix(self.user_item_matrix.values)

        # Use SVD for matrix factorization
        self.svd_model = TruncatedSVD(n_components=50, random_state=42)
        self.user_factors = self.svd_model.fit_transform(sparse_matrix)
        self.item_factors = self.svd_model.components_.T

        print(f"SVD model built with {self.svd_model.n_components} components")
        print(f"Explained variance ratio: {self.svd_model.explained_variance_ratio_.sum():.3f}")

    def get_movie_recommendations_by_title(self, movie_title, n_recommendations=10):
        """Get collaborative filtering recommendations for a movie title"""
        if movie_title not in self.title_to_movie_id:
            return f"Movie '{movie_title}' not found in ratings data"

        movie_id = self.title_to_movie_id[movie_title]

        # Get movie index in the matrix
        movie_idx = list(self.user_item_matrix.columns).index(movie_id)

        # Calculate item-item similarity using the factorized matrices
        movie_vector = self.item_factors[movie_idx].reshape(1, -1)
        similarities = cosine_similarity(movie_vector, self.item_factors).flatten()

        # Get top similar movies
        similar_indices = similarities.argsort()[::-1][1:n_recommendations+1]

        recommendations = []
        for idx in similar_indices:
            similar_movie_id = self.user_item_matrix.columns[idx]
            if similar_movie_id in self.movie_id_to_title:
                # Get movie details
                movie_info = self.movies_df[self.movies_df['id'] == similar_movie_id].iloc[0]

                # Calculate average rating from ratings data
                avg_rating = self.ratings_df[self.ratings_df['movieId'] == similar_movie_id]['rating'].mean()

                recommendations.append({
                    'title': self.movie_id_to_title[similar_movie_id],
                    'movie_id': similar_movie_id,
                    'similarity_score': similarities[idx],
                    'avg_rating': avg_rating,
                    'genres': movie_info['genres_list'],
                    'year': int(movie_info['release_year']) if pd.notna(movie_info['release_year']) else 'Unknown'
                })

        return recommendations

    def get_popular_movies(self, n_recommendations=10):
        """Get popular movies based on collaborative filtering data"""
        # Calculate popularity score (average rating * number of ratings)
        movie_stats = self.ratings_df.groupby('movieId').agg({
            'rating': ['mean', 'count']
        }).round(2)

        movie_stats.columns = ['avg_rating', 'num_ratings']
        movie_stats = movie_stats.reset_index()

        # Filter movies with at least 50 ratings
        popular_movies = movie_stats[movie_stats['num_ratings'] >= 50].copy()
        popular_movies['popularity_score'] = popular_movies['avg_rating'] * np.log(popular_movies['num_ratings'])
        popular_movies = popular_movies.sort_values('popularity_score', ascending=False)

        recommendations = []
        for _, movie_stat in popular_movies.head(n_recommendations).iterrows():
            movie_id = movie_stat['movieId']
            if movie_id in self.movie_id_to_title:
                movie_info = self.movies_df[self.movies_df['id'] == movie_id].iloc[0]
                recommendations.append({
                    'title': self.movie_id_to_title[movie_id],
                    'movie_id': movie_id,
                    'avg_rating': movie_stat['avg_rating'],
                    'num_ratings': movie_stat['num_ratings'],
                    'popularity_score': movie_stat['popularity_score'],
                    'genres': movie_info['genres_list'],
                    'year': int(movie_info['release_year']) if pd.notna(movie_info['release_year']) else 'Unknown'
                })

        return recommendations

# Initialize collaborative filtering recommender
print("="*60)
print("BUILDING COLLABORATIVE FILTERING RECOMMENDATION SYSTEM")
print("="*60)

collab_recommender = CollaborativeFilteringRecommender(user_item_matrix, movies_enhanced, ratings_filtered)

"""## 7. Hybrid Recommendation System"""

class HybridRecommender:
    def __init__(self, content_recommender, collab_recommender, movies_df):
        self.content_recommender = content_recommender
        self.collab_recommender = collab_recommender
        self.movies_df = movies_df

    def get_hybrid_recommendations(self, movie_title, n_recommendations=10,
                                 content_weight=0.6, collab_weight=0.4):
        """Get hybrid recommendations combining content-based and collaborative filtering"""
        print(f"Getting hybrid recommendations for '{movie_title}'")
        print(f"Weights: Content-based={content_weight}, Collaborative={collab_weight}")

        # Get recommendations from both methods
        content_recs = self.content_recommender.get_recommendations(movie_title, n_recommendations*2)
        collab_recs = self.collab_recommender.get_movie_recommendations_by_title(movie_title, n_recommendations*2)

        if isinstance(content_recs, str) or isinstance(collab_recs, str):
            # If one method fails, use the other
            if isinstance(content_recs, str) and not isinstance(collab_recs, str):
                print("Content-based failed, using collaborative filtering only")
                return collab_recs[:n_recommendations]
            elif isinstance(collab_recs, str) and not isinstance(content_recs, str):
                print("Collaborative filtering failed, using content-based only")
                return content_recs[:n_recommendations]
            else:
                return f"Both methods failed for movie '{movie_title}'"

        # Create a dictionary to combine scores
        combined_scores = {}

        # Add content-based recommendations
        for rec in content_recs:
            title = rec['title']
            combined_scores[title] = {
                'title': title,
                'content_score': rec['similarity_score'] * content_weight,
                'collab_score': 0,
                'genres': rec['genres'],
                'rating': rec['rating'],
                'year': rec['year']
            }

        # Add collaborative filtering recommendations
        for rec in collab_recs:
            title = rec['title']
            if title in combined_scores:
                combined_scores[title]['collab_score'] = rec['similarity_score'] * collab_weight
            else:
                combined_scores[title] = {
                    'title': title,
                    'content_score': 0,
                    'collab_score': rec['similarity_score'] * collab_weight,
                    'genres': rec['genres'],
                    'rating': rec['avg_rating'],
                    'year': rec['year']
                }

        # Calculate combined scores
        for title in combined_scores:
            combined_scores[title]['hybrid_score'] = (
                combined_scores[title]['content_score'] +
                combined_scores[title]['collab_score']
            )

        # Sort by hybrid score
        sorted_recs = sorted(combined_scores.values(),
                           key=lambda x: x['hybrid_score'], reverse=True)

        return sorted_recs[:n_recommendations]

    def get_hybrid_query_recommendations(self, query, n_recommendations=10):
        """Get hybrid recommendations for a text query"""
        print(f"Getting hybrid recommendations for query: '{query}'")

        # Get content-based recommendations for the query
        content_recs = self.content_recommender.get_recommendations_by_query(query, n_recommendations*2)

        # Get popular movies as baseline
        popular_recs = self.collab_recommender.get_popular_movies(n_recommendations*2)

        # Combine recommendations with content-based having higher weight for queries
        combined_scores = {}

        # Add content-based recommendations (70% weight)
        for rec in content_recs:
            title = rec['title']
            combined_scores[title] = {
                'title': title,
                'content_score': rec['similarity_score'] * 0.7,
                'popularity_score': 0,
                'genres': rec['genres'],
                'rating': rec['rating'],
                'year': rec['year']
            }

        # Add popularity scores (30% weight)
        for rec in popular_recs:
            title = rec['title']
            norm_popularity = min(rec['popularity_score'] / 100, 1.0)

            if title in combined_scores:
                combined_scores[title]['popularity_score'] = norm_popularity * 0.3
            else:
                combined_scores[title] = {
                    'title': title,
                    'content_score': 0,
                    'popularity_score': norm_popularity * 0.3,
                    'genres': rec['genres'],
                    'rating': rec['avg_rating'],
                    'year': rec['year']
                }

        # Calculate combined scores
        for title in combined_scores:
            combined_scores[title]['hybrid_score'] = (
                combined_scores[title]['content_score'] +
                combined_scores[title]['popularity_score']
            )

        # Sort by hybrid score
        sorted_recs = sorted(combined_scores.values(),
                           key=lambda x: x['hybrid_score'], reverse=True)

        # Filter out movies with zero hybrid score
        filtered_recs = [rec for rec in sorted_recs if rec['hybrid_score'] > 0]

        return filtered_recs[:n_recommendations]

# Initialize hybrid recommender
print("="*60)
print("BUILDING HYBRID RECOMMENDATION SYSTEM")
print("="*60)

hybrid_recommender = HybridRecommender(content_recommender, collab_recommender, movies_enhanced)
print("Hybrid recommender initialized successfully!")

"""## 8. Comprehensive Method Comparison"""

# Comprehensive comparison of all methods
print("="*80)
print("COMPREHENSIVE COMPARISON OF RECOMMENDATION METHODS")
print("="*80)

test_queries = [
    "horror movie suitable for teenagers",
    "action adventure movie",
    "romantic comedy",
    "war military movie",
    "animated family movie"
]

test_movie = "Toy Story"

print(f"\n{'='*50}")
print(f"MOVIE-BASED RECOMMENDATIONS FOR: {test_movie}")
print(f"{'='*50}")

# Content-based recommendations
print("\n1. CONTENT-BASED FILTERING")
print("-" * 30)
content_recs = content_recommender.get_recommendations(test_movie, 3)
if isinstance(content_recs, str):
    print(content_recs)
else:
    for i, rec in enumerate(content_recs, 1):
        print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
        print(f"   Similarity: {rec['similarity_score']:.3f} | Genres: {', '.join(rec['genres'][:2])}")

# Collaborative filtering recommendations
print("\n2. COLLABORATIVE FILTERING")
print("-" * 30)
collab_recs = collab_recommender.get_movie_recommendations_by_title(test_movie, 3)
if isinstance(collab_recs, str):
    print(collab_recs)
else:
    for i, rec in enumerate(collab_recs, 1):
        print(f"{i}. {rec['title']} ({rec['year']}) - Avg Rating: {rec['avg_rating']:.1f}")
        print(f"   Similarity: {rec['similarity_score']:.3f} | Genres: {', '.join(rec['genres'][:2])}")

# Hybrid recommendations
print("\n3. HYBRID APPROACH")
print("-" * 30)
hybrid_recs = hybrid_recommender.get_hybrid_recommendations(test_movie, 3)
if isinstance(hybrid_recs, str):
    print(hybrid_recs)
else:
    for i, rec in enumerate(hybrid_recs, 1):
        print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
        # Handle missing hybrid_score key
        score_key = 'hybrid_score' if 'hybrid_score' in rec else 'similarity_score'
        score_value = rec.get(score_key, 0)
        print(f"   Hybrid Score: {score_value:.3f} | Genres: {', '.join(rec['genres'][:2])}")

print(f"\n\n{'='*60}")
print("QUERY-BASED RECOMMENDATIONS COMPARISON")
print(f"{'='*60}")

for query in test_queries:
    print(f"\n\n🔍 Query: '{query}'")
    print("="*50)

    # Original search
    print("\n1. ORIGINAL SEARCH")
    print("-" * 20)
    original_results = search_movies(query, movies_enhanced)
    if original_results.get('results'):
        top_3 = original_results['results'][:3]
        for i, movie in enumerate(top_3, 1):
            release_date = movie['release_date'][:10] if movie['release_date'] != 'NaT' else 'Unknown'
            print(f"{i}. {movie['title']} ({release_date}) - Rating: {movie['rating']}")
    else:
        print("No results found")

    # Content-based
    print("\n2. CONTENT-BASED FILTERING")
    print("-" * 25)
    try:
        content_query_recs = content_recommender.get_recommendations_by_query(query, 3)
        if content_query_recs:
            for i, rec in enumerate(content_query_recs, 1):
                print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
                print(f"   Similarity: {rec['similarity_score']:.3f}")
        else:
            print("No results found")
    except Exception as e:
        print(f"Error in content-based filtering: {e}")

    # Hybrid approach
    print("\n3. HYBRID APPROACH")
    print("-" * 18)
    try:
        hybrid_query_recs = hybrid_recommender.get_hybrid_query_recommendations(query, 3)
        if hybrid_query_recs:
            for i, rec in enumerate(hybrid_query_recs, 1):
                print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
                # Handle missing hybrid_score key
                score_value = rec.get('hybrid_score', rec.get('similarity_score', 0))
                print(f"   Hybrid Score: {score_value:.3f}")
        else:
            print("No results found")
    except Exception as e:
        print(f"Error in hybrid approach: {e}")

print(f"\n\n{'='*50}")
print("ANALYSIS SUMMARY")
print(f"{'='*50}")
print("Successfully implemented:")
print("1. Content-Based Filtering using TF-IDF on movie features")
print("2. Collaborative Filtering using SVD matrix factorization")
print("3. Hybrid approach combining both methods")
print("4. Comparison with original keyword-based search")
print("\nKey Improvements over original search:")
print("- Semantic understanding of queries (not just keywords)")
print("- Consideration of cast, director, plot, and keywords")
print("- User preference patterns from collaborative filtering")
print("- Robust hybrid scoring combining multiple signals")

"""## 9. Conclusion and Recommendations

### Summary of Implementation

This notebook successfully implemented and compared four different movie recommendation approaches:

1. **Original Search**: Simple keyword-based genre filtering
2. **Content-Based Filtering**: TF-IDF vectorization of movie features (genres, overview, keywords, cast, director)
3. **Collaborative Filtering**: SVD matrix factorization on user ratings data
4. **Hybrid System**: Weighted combination of content-based and collaborative filtering

### Key Findings

- **Content-based filtering** provides the most versatile search experience, understanding semantic queries
- **Collaborative filtering** excels at finding movies with similar user preferences but has cold start issues
- **Hybrid approach** offers the best balance by combining strengths of both methods
- **Enhanced search** provides a backwards-compatible upgrade to the existing system

### Implementation Recommendations

1. **For Production**: Use the hybrid system as it provides the most robust recommendations
2. **For Backwards Compatibility**: Implement enhanced search to improve existing functionality
3. **For New Features**: Content-based filtering enables semantic search capabilities
4. **For User Personalization**: Collaborative filtering when user interaction data is available

The hybrid recommendation system successfully addresses the limitations of the original search while providing more relevant and diverse movie recommendations.

## 10. Netflix Assignment Evaluation - Test Cases from README

This section specifically evaluates the failing queries mentioned in the Netflix assignment README to demonstrate quantitative improvement.
"""

# Netflix Assignment Test Suite
test_queries = [
    "A horror movie suitable for teenagers in the 12-15 age range",
    "A spooky movie suitable for teenagers in the 12-15 age range",
    "War commandos",
    "scary movie for teens",
    "military commandos"
]

results = {'Original Search': [], 'Content-Based': [], 'Hybrid': []}

for query in test_queries:
    original_count = len(search_movies(query, movies_enhanced).get('results', []))
    content_count = len(content_recommender.get_recommendations_by_query(query, 5))
    hybrid_count = len(hybrid_recommender.get_hybrid_query_recommendations(query, 5))

    results['Original Search'].append(original_count > 0)
    results['Content-Based'].append(content_count > 0)
    results['Hybrid'].append(hybrid_count > 0)

    print(f"Query: {query}")
    print(f"  Original: {original_count} results")
    print(f"  Content:  {content_count} results")
    print(f"  Hybrid:   {hybrid_count} results")
    print()

# Success Rate Analysis
for method, result_list in results.items():
    success_rate = sum(result_list) / len(result_list) * 100
    print(f"{method}: {success_rate:.0f}% success rate")

readme_failures = [
    "A spooky movie suitable for teenagers in the 12-15 age range",
    "War commandos"
]

print("\nNetflix Assignment Key Fixes:")
for query in readme_failures:
    idx = test_queries.index(query)
    original = results['Original Search'][idx]
    content = results['Content-Based'][idx]
    hybrid = results['Hybrid'][idx]

    status = "FIXED" if not original and (content or hybrid) else "Still failing"
    print(f"'{query}': {status}")

netflix_queries = test_queries[:3]
original_score = sum([results['Original Search'][test_queries.index(q)] for q in netflix_queries])
content_score = sum([results['Content-Based'][test_queries.index(q)] for q in netflix_queries])
hybrid_score = sum([results['Hybrid'][test_queries.index(q)] for q in netflix_queries])

print(f"\nNetflix README Score:")
print(f"Original: {original_score}/3 queries working")
print(f"Content:  {content_score}/3 queries working")
print(f"Hybrid:   {hybrid_score}/3 queries working")

"""## 11. Enhanced Content-Based Filtering with Sentence Transformers

TF-IDF treats words independently and misses semantic context. This section implements a Sentence Transformers approach that understands semantic meaning.
"""

# Install sentence-transformers with fallback handling
try:
    from sentence_transformers import SentenceTransformer
    import torch
    print("Sentence Transformers already installed")
except ImportError:
    print("Installing sentence-transformers...")
    import subprocess
    subprocess.run(['pip', 'install', 'sentence-transformers'], check=True)
    from sentence_transformers import SentenceTransformer
    import torch
    print("Installation complete")

class EnhancedContentBasedRecommender:
    def __init__(self, movies_df):
        self.movies_df = movies_df.copy()

        # GPU detection and configuration
        self.device = self._setup_device()
        print(f"Using device: {self.device}")

        # Initialize model with device configuration
        self.model = SentenceTransformer('all-MiniLM-L12-v2')
        if self.device != 'cpu':
            self.model = self.model.to(self.device)

        self.content_embeddings = None
        self.movie_indices = None
        self._prepare_content_features()
        self._build_embedding_matrix()

    def _setup_device(self):
        """Detect and configure the best available device"""
        if torch.cuda.is_available():
            device = 'cuda'
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"GPU detected: {gpu_name} ({gpu_memory:.1f} GB)")
            return device
        else:
            print("No GPU detected, using CPU")
            return 'cpu'

    def _prepare_content_features(self):
        """Combine content features into text for embedding"""
        print("Preparing content features for semantic embedding...")

        def format_features(row):
            features = []
            if row['genres_list']: features.extend(row['genres_list'])
            if row['keywords_list']: features.extend(row['keywords_list'][:5])
            # if row['director']: features.append(f"directed by {row['director']}")
            # if row['cast_list']: features.append(f"starring {', '.join(row['cast_list'][:3])}")
            if row['overview']: features.append(row['overview'])
            return ' '.join(features)

        self.movies_df['content_text'] = self.movies_df.apply(format_features, axis=1)
        self.movie_indices = pd.Series(self.movies_df.index, index=self.movies_df['title']).to_dict()
        print(f"Prepared content features for {len(self.movies_df)} movies")

    def _build_embedding_matrix(self):
        """Build semantic embeddings matrix with GPU optimization"""
        print("Building semantic embeddings matrix...")

        # Configure batch size based on device
        if self.device == 'cuda':
            batch_size = 128  # Larger batch for GPU
            print("Using GPU-optimized batch processing")
        else:
            batch_size = 32  # Conservative batch for CPU
            print("Using CPU batch processing")

        texts = self.movies_df['content_text'].tolist()

        # Add timing and memory monitoring
        import time
        start_time = time.time()

        if self.device == 'cuda':
            # Monitor GPU memory
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()

        self.content_embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            device=self.device
        )

        encoding_time = time.time() - start_time

        if self.device == 'cuda':
            peak_memory = torch.cuda.max_memory_allocated()
            memory_used = (peak_memory - initial_memory) / 1e9
            print(f"GPU memory used: {memory_used:.2f} GB")
            torch.cuda.empty_cache()

        print(f"Embedding matrix shape: {self.content_embeddings.shape}")
        print(f"Encoding completed in {encoding_time:.2f}s ({encoding_time/len(texts)*1000:.1f}ms per text)")

    def get_recommendations(self, movie_title, n_recommendations=10):
        """Get semantic content-based recommendations"""
        if movie_title not in self.movie_indices:
            return f"Movie '{movie_title}' not found in database"

        movie_idx = self.movie_indices[movie_title]
        movie_embedding = self.content_embeddings[movie_idx].reshape(1, -1)

        # Calculate cosine similarity
        similarities = cosine_similarity(movie_embedding, self.content_embeddings).flatten()
        similar_indices = similarities.argsort()[::-1][1:n_recommendations+1]

        recommendations = []
        for idx in similar_indices:
            movie = self.movies_df.iloc[idx]
            recommendations.append({
                'title': movie['title'],
                'genres': movie['genres_list'],
                'rating': movie['vote_average'],
                'year': int(movie['release_year']) if pd.notna(movie['release_year']) else 'Unknown',
                'similarity_score': similarities[idx],
                'director': movie['director']
            })

        return recommendations

    def get_recommendations_by_query(self, query, n_recommendations=10):
        """Get semantic recommendations for text query"""
        print(f"Getting semantic recommendations for: '{query}'")

        # Encode query into same semantic space
        query_embedding = self.model.encode([query], device=self.device)
        similarities = cosine_similarity(query_embedding, self.content_embeddings).flatten()
        similar_indices = similarities.argsort()[::-1][:n_recommendations]

        recommendations = []
        for idx in similar_indices:
            if similarities[idx] > 0:
                movie = self.movies_df.iloc[idx]
                recommendations.append({
                    'title': movie['title'],
                    'genres': movie['genres_list'],
                    'rating': movie['vote_average'],
                    'year': int(movie['release_year']) if pd.notna(movie['release_year']) else 'Unknown',
                    'similarity_score': similarities[idx],
                    'director': movie['director']
                })

        return recommendations

print("Creating Enhanced Content-Based Recommender with GPU support...")
enhanced_recommender = EnhancedContentBasedRecommender(movies_enhanced)

# Direct TF-IDF vs Sentence Transformers Comparison
import time

comparison_queries = [
    "A spooky movie suitable for teenagers",
    "War commandos",
    "scary horror film",
    "military soldiers movie",
    "romantic comedy"
]

print("TF-IDF vs Sentence Transformers Comparison")
print("=" * 50)

for query in comparison_queries:
    print(f"\nQuery: '{query}'")
    print("-" * 30)

    # TF-IDF approach
    start_time = time.time()
    tfidf_results = content_recommender.get_recommendations_by_query(query, 3)
    tfidf_time = time.time() - start_time

    # Sentence Transformers approach
    start_time = time.time()
    semantic_results = enhanced_recommender.get_recommendations_by_query(query, 3)
    semantic_time = time.time() - start_time

    print(f"TF-IDF Results ({tfidf_time:.3f}s):")
    for i, rec in enumerate(tfidf_results, 1):
        print(f"  {i}. {rec['title']} ({rec['year']}) - Score: {rec['similarity_score']:.3f}")

    print(f"\nSentence Transformers Results ({semantic_time:.3f}s):")
    for i, rec in enumerate(semantic_results, 1):
        print(f"  {i}. {rec['title']} ({rec['year']}) - Score: {rec['similarity_score']:.3f}")

    print()

# Semantic Understanding Analysis
print("Semantic Understanding Test")
print("=" * 30)

# Test semantic similarity between related terms
semantic_tests = [
    ["spooky movie", "scary movie", "frightening film"],
    ["war commandos", "military soldiers", "combat troops"],
    ["teenage horror", "horror for teenagers", "teen scary movie"],
    ["romantic comedy", "rom-com", "funny love story"]
]

for test_group in semantic_tests:
    print(f"\nTesting semantic similarity: {test_group}")

    # Get embeddings for each term
    embeddings = enhanced_recommender.model.encode(test_group)

    # Calculate pairwise similarities
    similarities = cosine_similarity(embeddings)

    for i, term1 in enumerate(test_group):
        for j, term2 in enumerate(test_group):
            if i < j:  # Only show upper triangle
                sim_score = similarities[i][j]
                print(f"  '{term1}' vs '{term2}': {sim_score:.3f}")
    print()

class SemanticHybridRecommender:
    def __init__(self, semantic_recommender, collab_recommender, movies_df):
        self.semantic_recommender = semantic_recommender
        self.collab_recommender = collab_recommender
        self.movies_df = movies_df
        print("Semantic Hybrid Recommender initialized with GPU-optimized Sentence Transformers!")

    def get_hybrid_recommendations(self, movie_title, n_recommendations=10,
                                 semantic_weight=0.8, collab_weight=0.2):
        """Get hybrid recommendations combining semantic content-based and collaborative filtering"""
        print(f"Getting semantic hybrid recommendations for '{movie_title}'")
        print(f"Weights: Semantic={semantic_weight}, Collaborative={collab_weight}")

        # Get recommendations from both methods
        semantic_recs = self.semantic_recommender.get_recommendations(movie_title, n_recommendations*2)
        collab_recs = self.collab_recommender.get_movie_recommendations_by_title(movie_title, n_recommendations*2)

        if isinstance(semantic_recs, str) or isinstance(collab_recs, str):
            # If one method fails, use the other
            if isinstance(semantic_recs, str) and not isinstance(collab_recs, str):
                print("Semantic content-based failed, using collaborative filtering only")
                return collab_recs[:n_recommendations]
            elif isinstance(collab_recs, str) and not isinstance(semantic_recs, str):
                print("Collaborative filtering failed, using semantic content-based only")
                return semantic_recs[:n_recommendations]
            else:
                return f"Both methods failed for movie '{movie_title}'"

        # Create a dictionary to combine scores
        combined_scores = {}

        # Add semantic content-based recommendations
        for rec in semantic_recs:
            title = rec['title']
            combined_scores[title] = {
                'title': title,
                'semantic_score': rec['similarity_score'] * semantic_weight,
                'collab_score': 0,
                'genres': rec['genres'],
                'rating': rec['rating'],
                'year': rec['year'],
                'director': rec.get('director', '')
            }

        # Add collaborative filtering recommendations
        for rec in collab_recs:
            title = rec['title']
            if title in combined_scores:
                combined_scores[title]['collab_score'] = rec['similarity_score'] * collab_weight
            else:
                combined_scores[title] = {
                    'title': title,
                    'semantic_score': 0,
                    'collab_score': rec['similarity_score'] * collab_weight,
                    'genres': rec['genres'],
                    'rating': rec['avg_rating'],
                    'year': rec['year'],
                    'director': ''
                }

        # Calculate combined scores
        for title in combined_scores:
            combined_scores[title]['hybrid_score'] = (
                combined_scores[title]['semantic_score'] +
                combined_scores[title]['collab_score']
            )

        # Sort by hybrid score
        sorted_recs = sorted(combined_scores.values(),
                           key=lambda x: x['hybrid_score'], reverse=True)

        return sorted_recs[:n_recommendations]

    def get_hybrid_query_recommendations(self, query, n_recommendations=10):
        """Get hybrid recommendations for a text query using semantic understanding"""
        print(f"Getting semantic hybrid recommendations for query: '{query}'")
        print(f"Using GPU-accelerated semantic processing on device: {self.semantic_recommender.device}")

        # Get semantic content-based recommendations for the query
        semantic_recs = self.semantic_recommender.get_recommendations_by_query(query, n_recommendations*2)

        # Get popular movies as baseline from collaborative filtering
        popular_recs = self.collab_recommender.get_popular_movies(n_recommendations*2)

        # Combine recommendations with semantic having higher weight for queries (better context understanding)
        combined_scores = {}

        # Add semantic content-based recommendations (75% weight for better semantic understanding)
        for rec in semantic_recs:
            title = rec['title']
            combined_scores[title] = {
                'title': title,
                'semantic_score': rec['similarity_score'] * 0.75,
                'popularity_score': 0,
                'genres': rec['genres'],
                'rating': rec['rating'],
                'year': rec['year'],
                'director': rec.get('director', '')
            }

        # Add popularity scores (25% weight)
        for rec in popular_recs:
            title = rec['title']
            norm_popularity = min(rec['popularity_score'] / 100, 1.0)

            if title in combined_scores:
                combined_scores[title]['popularity_score'] = norm_popularity * 0.25
            else:
                combined_scores[title] = {
                    'title': title,
                    'semantic_score': 0,
                    'popularity_score': norm_popularity * 0.25,
                    'genres': rec['genres'],
                    'rating': rec['avg_rating'],
                    'year': rec['year'],
                    'director': ''
                }

        # Calculate combined scores
        for title in combined_scores:
            combined_scores[title]['hybrid_score'] = (
                combined_scores[title]['semantic_score'] +
                combined_scores[title]['popularity_score']
            )

        # Sort by hybrid score
        sorted_recs = sorted(combined_scores.values(),
                           key=lambda x: x['hybrid_score'], reverse=True)

        # Filter out movies with zero hybrid score
        filtered_recs = [rec for rec in sorted_recs if rec['hybrid_score'] > 0]

        return filtered_recs[:n_recommendations]

# Initialize semantic hybrid recommender
print("="*60)
print("BUILDING SEMANTIC HYBRID RECOMMENDATION SYSTEM")
print("="*60)

semantic_hybrid_recommender = SemanticHybridRecommender(
    enhanced_recommender,
    collab_recommender,
    movies_enhanced
)

# Test the semantic hybrid system
print("\nTesting Semantic Hybrid Recommendations")
print("="*45)

# Test with movie-based recommendations
test_movie = "Toy Story"
print(f"\nSemantic Hybrid recommendations for '{test_movie}':")
print("-" * 45)
semantic_hybrid_recs = semantic_hybrid_recommender.get_hybrid_recommendations(test_movie, 5)
if isinstance(semantic_hybrid_recs, str):
    print(semantic_hybrid_recs)
else:
    for i, rec in enumerate(semantic_hybrid_recs, 1):
        print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
        score_value = rec.get('hybrid_score', 0)
        print(f"   Hybrid Score: {score_value:.3f} | Genres: {', '.join(rec['genres'][:2])}")

# Test with query-based recommendations for Netflix failing queries
netflix_test_queries = [
    "A spooky movie suitable for teenagers in the 12-15 age range",
    "War commandos"
]

for query in netflix_test_queries:
    print(f"\nSemantic Hybrid results for: '{query}'")
    print("-" * 50)
    semantic_hybrid_results = semantic_hybrid_recommender.get_hybrid_query_recommendations(query, 3)
    if semantic_hybrid_results:
        for i, rec in enumerate(semantic_hybrid_results, 1):
            print(f"{i}. {rec['title']} ({rec['year']}) - Rating: {rec['rating']:.1f}")
            print(f"   Hybrid Score: {rec['hybrid_score']:.3f}")
    else:
        print("No recommendations found")
    print()

